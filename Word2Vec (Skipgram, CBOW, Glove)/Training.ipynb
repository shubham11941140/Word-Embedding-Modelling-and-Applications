{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mbjSnJoY1Ek",
        "outputId": "34f70c35-a3d7-4ece-a8a4-9c666d75a513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwWbjv_tey_H",
        "outputId": "a46cdc2c-7956-4419-929b-66851c2ae7fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9gaQRc7BZUny"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import gc\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from glob import glob\n",
        "from tqdm import tqdm, trange\n",
        "from torch import optim\n",
        "from collections import Counter, defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VwmWxcJPZw6Z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92AX1TS4ZWoT",
        "outputId": "341d731f-e7f5-4ed4-f9e0-fede7e19896c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q7NI9fwFZnUh"
      },
      "outputs": [],
      "source": [
        "lem = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(list(string.punctuation)) # punctutation\n",
        "stop_words.update([\"''\", \"``\"]) # extra based on corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9KDARqmUZYvv"
      },
      "outputs": [],
      "source": [
        "txt_dir='/content/drive/MyDrive/NLP/text'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLRNRJTaZc0s"
      },
      "outputs": [],
      "source": [
        "txt_files = glob(os.path.join(f\"{txt_dir}/\", \"*.txt\"))\n",
        "print(txt_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTJwGJmwZrnz"
      },
      "outputs": [],
      "source": [
        "for text in txt_files:\n",
        "    with open(text,'r+') as f :\n",
        "        lines,res = f.readlines(),[]\n",
        "        for line in tqdm(lines, total = len(lines)):\n",
        "            s_line = sent_tokenize(line)\n",
        "            for l in s_line : \n",
        "                res.append(\" \".join([lem.lemmatize(w.lower(), 'v') for w in word_tokenize(l) if w not in stop_words]))\n",
        "        final = '\\n'.join(res)\n",
        "        f.seek(0)\n",
        "        f.write(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vUO8yLFfbH-o"
      },
      "outputs": [],
      "source": [
        "# remove words from the vocabulary with less than a particular frequency \n",
        "# create a <unk> token\n",
        "\n",
        "def gather_word_freqs(collection, sampling_freq):\n",
        "\n",
        "    vocab, word_idx, idx_word = Counter(),{},{}\n",
        "    new_vocab = Counter()\n",
        "    sampling_rate = 0.001\n",
        "    word_counter = 0\n",
        "    \n",
        "    for sent in tqdm(collection, total = len(collection)):\n",
        "        vocab.update(sent)\n",
        "      \n",
        "    for k,v in vocab.items():\n",
        "        if v >= sampling_freq:\n",
        "            new_vocab[k] = v\n",
        "      \n",
        "    new_vocab[\"<unk>\"] = 0\n",
        "\n",
        "    for i,(k,v) in enumerate(new_vocab.items()):\n",
        "        word_idx[k] = i\n",
        "        idx_word[i] = k\n",
        "   \n",
        "    return collection, new_vocab, word_idx, idx_word\n",
        "\n",
        "def gather_training_data(collection, word_idx, vocab, cs, embed_type):\n",
        "    training_data = []\n",
        "    coo_counts = Counter()\n",
        "    \n",
        "    for sent in tqdm(collection, total = len(collection)):\n",
        "\n",
        "        indices = []\n",
        "        for w in sent:\n",
        "          if w not in word_idx:\n",
        "            indices.append(0)\n",
        "          else:\n",
        "            indices.append(word_idx[w])\n",
        "        \n",
        "        if embed_type == 'skipgram' or embed_type == \"glove\":\n",
        "            for i in range(len(indices)):\n",
        "                for j in range(max(-cs, 0), min(cs + 1, len(indices))):\n",
        "                    if i == j : \n",
        "                        continue \n",
        "                    training_data.append((indices[i], indices[j]))\n",
        "        else : \n",
        "            if len(indices) < (2*cs + 1): \n",
        "                continue\n",
        "\n",
        "            for i in range(cs, len(indices) - cs) : \n",
        "                context = []\n",
        "                for j in range(i-cs, i+cs):\n",
        "                    if i == j : \n",
        "                        continue \n",
        "                    context.append(indices[j])\n",
        "                training_data.append((context, indices[i]))\n",
        "\n",
        "        if embed_type == \"glove\":\n",
        "            coo_counts.update(training_data)\n",
        "            tokens = defaultdict(lambda: -1)\n",
        "            for word, count in vocab.most_common(len(vocab)):\n",
        "                if count >= 5 : \n",
        "                    tokens[word_idx[word]] = count\n",
        "            \n",
        "            training_data = [(w[0],w[1],count) for w, count in coo_counts.items() if tokens[w[0]]> 0 and tokens[w[1]] > 0]\n",
        "    \n",
        "    return training_data\n",
        "\n",
        "def load_data(txt_dir,cs,embed_type):\n",
        "        \n",
        "    collection = []\n",
        "    for txt in txt_dir:\n",
        "        with open(txt,'r') as f : \n",
        "            lines = f.readlines()\n",
        "            collection += [line.split() for line in tqdm(lines, total = len(lines))]  \n",
        "\n",
        "    (collection, vocab, word_idx, idx_word) = gather_word_freqs(collection, 100)\n",
        "\n",
        "    training_data = gather_training_data(collection, word_idx, vocab, cs,embed_type)\n",
        "    \n",
        "    return vocab, word_idx, idx_word, training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CjYFNOf7EyGm"
      },
      "outputs": [],
      "source": [
        "class Skipgram(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size):\n",
        "        super(Skipgram, self).__init__()\n",
        "        # we first start with the lookup(Embedding layer) layer for skipgram\n",
        "        self.embeddings_input = nn.Embedding(vocab_size,embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size,vocab_size) #Ax+B layer\n",
        "        \n",
        "    def forward(self, input_word):\n",
        "        embeds = self.embeddings_input(input_word)\n",
        "        out = self.linear(embeds)\n",
        "        log_prob = F.log_softmax(out, dim = 1)\n",
        "        return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QpjDftp0Ez9x"
      },
      "outputs": [],
      "source": [
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings_input = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "        \n",
        "    def forward(self, input_word):\n",
        "        embeds = self.embeddings_input(input_word)\n",
        "        embeds = torch.sum(embeds, dim = 1)\n",
        "        out = self.linear(embeds)\n",
        "        log_prob = F.log_softmax(out, dim = 1)\n",
        "        return log_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gdY8wOR-E1_s"
      },
      "outputs": [],
      "source": [
        "class GloVeModel(nn.Module):\n",
        "    def __init__(self, embedding_size, vocab_size, x_max):\n",
        "        super(GloVeModel, self).__init__()\n",
        "        self.x_max = x_max\n",
        "        self._focal_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self._context_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
        "        self._focal_biases = nn.Embedding(vocab_size,1).type(torch.float64)\n",
        "        self._context_biases = nn.Embedding(vocab_size,1).type(torch.float64)\n",
        "        \n",
        "    def forward(self, focal_input, context_input, coocurrence_count):\n",
        "        x_max = max(self.x_max, 1)\n",
        "        focal_embed = self._focal_embeddings(focal_input)\n",
        "        context_embed = self._context_embeddings(context_input)\n",
        "        focal_bias = self._focal_biases(focal_input)\n",
        "        context_bias = self._context_biases(context_input)\n",
        "        \n",
        "        weight_factor = torch.pow(coocurrence_count/x_max, 0.75)\n",
        "        weight_factor[weight_factor>1] = 1\n",
        "        \n",
        "        embedding_products = torch.sum(focal_embed*context_embed, dim = 1)\n",
        "        log_cooccurrences = torch.log(coocurrence_count)\n",
        "        \n",
        "        distance_expr = (embedding_products + focal_bias + context_bias + log_cooccurrences)**2\n",
        "        \n",
        "        single_losses = weight_factor*distance_expr\n",
        "        mean_loss = torch.mean(single_losses)\n",
        "        \n",
        "        return mean_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QBgz-qRlGCkN"
      },
      "outputs": [],
      "source": [
        "class NEGLoss(nn.Module):\n",
        "    def __init__(self, ix_to_word, word_freqs, num_negative_samples=5,):\n",
        "        super(NEGLoss, self).__init__()\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.num_words = len(ix_to_word)\n",
        "        self.distr = F.normalize(torch.Tensor(\n",
        "            [word_freqs[ix_to_word[i]] for i in range(len(word_freqs))]).pow(0.75), dim=0\n",
        "        )\n",
        "\n",
        "    def sample(self, num_samples, positives=[]):\n",
        "        weights = torch.zeros((self.num_words, 1))\n",
        "        for w in positives: weights[w] += 1.0\n",
        "        for _ in range(num_samples):\n",
        "            w = torch.multinomial(self.distr, 1)[0]\n",
        "            while (w in positives):\n",
        "                w = torch.multinomial(self.distr, 1)[0]\n",
        "            weights[w] += 1.0\n",
        "        return weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        return F.nll_loss(input, target,\n",
        "            self.sample(self.num_negative_samples, positives=target.cpu().data.numpy()).squeeze().to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "adxs9ppOOeNy"
      },
      "outputs": [],
      "source": [
        "def train(txt_dir, epochs = 5, embed_dim = 300, cs = 2, model_type = \"skipgram\", negative_sampling = False, device = 'cpu'):\n",
        "\n",
        "  if model_type is \"skipgram\":\n",
        "    vocab, word_idx,idx_word,training_data = load_data(txt_dir,cs,\"skipgram\")\n",
        "    model = Skipgram(embed_dim, len(vocab))\n",
        "  elif model_type is \"cbow\":\n",
        "    vocab, word_idx,idx_word,training_data = load_data(txt_dir,cs,\"cbow\")\n",
        "    model = CBOWModel(embed_dim, len(vocab))\n",
        "  else:\n",
        "    vocab, word_idx,idx_word,training_data = load_data(txt_dir,cs,\"glove\")\n",
        "    model = GloVeModel(embed_dim, len(vocab), negative_sampling)\n",
        "\n",
        "  if negative_sampling:\n",
        "      loss_function = NEGLoss(idx_word, vocab)\n",
        "  else:\n",
        "      loss_function = nn.NLLLoss()\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "  losses = []\n",
        "\n",
        "  st = time.time()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      total_loss = 0\n",
        "      print(\"Beginning epoch %d\" % epoch)\n",
        "      print(\"Length Training Data\", len(training_data))\n",
        "\n",
        "      if model_type == 'glove' : \n",
        "        for (focal, context, counts) in training_data:\n",
        "            context_var = torch.LongTensor([context]).to(device)\n",
        "            focal = torch.LongTensor([focal]).to(device)\n",
        "            counts = torch.LongTensor([counts]).to(device)\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(focal, context_var, counts)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "      else:\n",
        "                \n",
        "        for context, target in tqdm(training_data, total=len(training_data)):\n",
        "            context_var = torch.LongTensor([context]).to(device)\n",
        "            target =  torch.LongTensor([target]).to(device)\n",
        "            model.to(device)\n",
        "            model.train()\n",
        "            optimizer.zero_grad()\n",
        "            log_probs = model(context_var)\n",
        "            loss = loss_function(log_probs, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss\n",
        "        losses.append(total_loss)\n",
        "\n",
        "  torch.save(\n",
        "        {\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"losses\": losses,\n",
        "            \"word_idx\": word_idx,\n",
        "            \"idx_word\": idx_word,\n",
        "        },\n",
        "        f\"models/model_{model_type}_neg{negative_sampling}.pth\",\n",
        "  )\n",
        "  print(\"MODEL SAVED\")\n",
        "\n",
        "  et = time.time()\n",
        "  # print(\"time taken\", (et - st))\n",
        "  return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SULzoFZhAnon"
      },
      "outputs": [],
      "source": [
        "txt_dir = [\"opaa\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_zoAJxQFMsp"
      },
      "outputs": [],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"glove\", negative_sampling = False, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXYVIiZfYGhl"
      },
      "outputs": [],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"glove\", negative_sampling = 5, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA-VmBkGablV"
      },
      "outputs": [],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"cbow\", negative_sampling = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdWcCnVLPaAt",
        "outputId": "19156924-4791-4204-e100-53a75d2e49d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 426695.35it/s]\n",
            "100%|██████████| 16786/16786 [00:00<00:00, 230934.15it/s]\n",
            "100%|██████████| 16786/16786 [00:00<00:00, 33800.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 0\n",
            "Length Training Data 965509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 14.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 1\n",
            "Length Training Data 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 451.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 2\n",
            "Length Training Data 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 453.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 3\n",
            "Length Training Data 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 449.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 4\n",
            "Length Training Data 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 445.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL SAVED\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[tensor(554.6580, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(463.8362, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(396.1562, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(360.1760, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(342.4267, device='cuda:0', grad_fn=<AddBackward0>)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"skipgram\", negative_sampling = True, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezAe3bjaIWDp",
        "outputId": "32d4b9c8-d5dc-4a3f-b08f-bb4dc03f34f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 411386.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 242344.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27777777777777777777777777777777777777777777 246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 16903.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL DONE\n",
            "NEGATIVE LOSS DONE\n",
            "Beginning epoch 0\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [10:09<00:00, 453.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 1\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [10:11<00:00, 451.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 2\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [10:05<00:00, 456.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 3\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [10:09<00:00, 453.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 4\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [10:12<00:00, 451.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL SAVED\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[tensor(679761.7500, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(648273.1250, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(641574.6875, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(638099.1875, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(635637.6250, device='cuda:0', grad_fn=<AddBackward0>)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"cbow\", negative_sampling = True, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJEzVTXkPeWe",
        "outputId": "8094d70a-b3d4-47ee-8cea-018aa09a93ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 330510.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 185955.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27777777777777777777777777777777777777777777 246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16786/16786 [00:00<00:00, 18608.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL DONE\n",
            "NEGATIVE LOSS DONE\n",
            "Beginning epoch 0\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [04:18<00:00, 1070.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 1\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [04:08<00:00, 1111.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 2\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [04:06<00:00, 1122.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 3\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [04:06<00:00, 1118.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beginning epoch 4\n",
            "Length Training Data 276130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 276130/276130 [04:06<00:00, 1121.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MODEL SAVED\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[tensor(681882., device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(649145., device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(642236.3750, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(638825.5000, device='cuda:0', grad_fn=<AddBackward0>),\n",
              " tensor(636526.8125, device='cuda:0', grad_fn=<AddBackward0>)]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train(txt_dir, epochs = 5, embed_dim = 100, cs = 2, model_type = \"cbow\", negative_sampling = False, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrAIxPFZwkAY",
        "outputId": "ab609085-4a36-4fe4-b259-41382eb570ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'model_state_dict': OrderedDict([('embeddings_input.weight', tensor([[ 0.1146,  1.0594, -0.2355,  ...,  0.2623,  0.0352, -0.3516],\n",
            "        [ 0.6221, -0.5655, -1.8811,  ...,  1.6409,  0.7659, -0.0264],\n",
            "        [-0.9397, -0.7928,  0.1202,  ..., -0.8976, -1.5699, -0.4129],\n",
            "        ...,\n",
            "        [-0.4142, -0.8826, -0.6462,  ...,  1.2085, -0.3424,  0.2064],\n",
            "        [ 1.8511,  0.0413,  1.3784,  ..., -1.6358,  0.6703,  1.6557],\n",
            "        [ 1.8665, -0.8989,  0.7596,  ...,  1.0907, -0.2864, -1.4045]],\n",
            "       device='cuda:0')), ('linear.weight', tensor([[ 0.0196,  0.1580, -0.0417,  ..., -0.0100, -0.0564,  0.0432],\n",
            "        [ 0.0560,  0.0138,  0.0629,  ..., -0.0948, -0.0449,  0.0696],\n",
            "        [ 0.0305,  0.0905,  0.0163,  ..., -0.0414, -0.0616, -0.0765],\n",
            "        ...,\n",
            "        [-0.0931,  0.0216,  0.0804,  ...,  0.0765, -0.0265, -0.0260],\n",
            "        [-0.0793, -0.0077,  0.0575,  ...,  0.0188,  0.0252, -0.0913],\n",
            "        [ 0.0016, -0.0434, -0.0680,  ..., -0.0712,  0.0537, -0.0935]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([ 0.0522, -0.0697,  0.0581, -0.0082,  0.0437,  0.0582,  0.0679,  0.0829,\n",
            "         0.0443,  0.0677, -0.0020, -0.0649,  0.0571, -0.0005, -0.0356,  0.0165,\n",
            "         0.0152,  0.0801, -0.0786,  0.1020, -0.0331, -0.0710,  0.0006, -0.0149,\n",
            "        -0.0872, -0.0372,  0.0409, -0.0253,  0.0231, -0.0148, -0.0903, -0.0858,\n",
            "         0.0574,  0.0263, -0.0573,  0.0163, -0.0575,  0.0355, -0.0770, -0.0068,\n",
            "         0.0467,  0.0839, -0.0889, -0.0403,  0.0790, -0.0409,  0.0474, -0.0709,\n",
            "         0.0555,  0.0816, -0.0358, -0.0754,  0.0571,  0.0749, -0.0905,  0.0126,\n",
            "         0.0922,  0.0006, -0.0411, -0.0450,  0.0302, -0.0693,  0.0451, -0.0363,\n",
            "         0.0853,  0.0348,  0.0770, -0.0685,  0.0151,  0.0750,  0.0399,  0.0677,\n",
            "        -0.0464,  0.0022, -0.0449, -0.0796, -0.0927,  0.0506,  0.0202,  0.0954,\n",
            "         0.0849, -0.0918,  0.0278,  0.0087, -0.0167,  0.0712, -0.0528, -0.0540,\n",
            "        -0.0667, -0.0323,  0.0223,  0.0244,  0.0945,  0.0788, -0.0026, -0.0060,\n",
            "        -0.0070, -0.0427,  0.0615, -0.0793, -0.0147, -0.0755,  0.0756,  0.0549,\n",
            "        -0.0740,  0.0596, -0.0313, -0.0631,  0.0727,  0.0580,  0.0151,  0.0093,\n",
            "        -0.0109, -0.0623,  0.0700, -0.0383,  0.0679,  0.0889,  0.0711, -0.0996,\n",
            "         0.0059,  0.0397,  0.0420, -0.0706, -0.0509, -0.0103, -0.0435,  0.0713,\n",
            "        -0.0890, -0.0244, -0.0784, -0.0754, -0.0898,  0.0140, -0.0274, -0.0591,\n",
            "        -0.0519,  0.0837, -0.0867,  0.0391,  0.0610, -0.0083,  0.0906, -0.0974,\n",
            "        -0.0126,  0.0689,  0.0805,  0.0841, -0.0860,  0.0578, -0.0427,  0.0511,\n",
            "         0.0081,  0.0013, -0.0183, -0.0614, -0.0462,  0.0756, -0.1003, -0.0638,\n",
            "         0.0910, -0.0530, -0.0952,  0.0443,  0.0199,  0.0169, -0.0073, -0.0229,\n",
            "        -0.0310,  0.0863,  0.0231,  0.0789,  0.0013, -0.0744, -0.0057, -0.0027,\n",
            "         0.0298, -0.0040,  0.0765,  0.0697,  0.0759, -0.0383, -0.0728,  0.0138,\n",
            "        -0.0677, -0.0659,  0.0581, -0.0053,  0.0094, -0.0679,  0.0194, -0.0791,\n",
            "         0.0966,  0.0738,  0.0145,  0.0679,  0.0518, -0.0143, -0.0981,  0.0622,\n",
            "        -0.0208, -0.0350,  0.0584,  0.0970,  0.0388, -0.0699,  0.0776, -0.0388,\n",
            "         0.0865, -0.0839, -0.0685,  0.0504, -0.0994, -0.0997, -0.0015, -0.0084,\n",
            "        -0.0991, -0.0808,  0.0880, -0.0997, -0.0363,  0.0257, -0.0159,  0.0830,\n",
            "        -0.0584, -0.0822, -0.0944, -0.0875,  0.0482,  0.0848, -0.0377, -0.1001,\n",
            "        -0.0953,  0.0626, -0.0584, -0.0561,  0.0152, -0.0443,  0.0219, -0.0011,\n",
            "        -0.0393,  0.0901,  0.0004, -0.0434,  0.0114, -0.0293], device='cuda:0'))]), 'losses': [tensor(258.2772, device='cuda:0', requires_grad=True), tensor(208.1445, device='cuda:0', requires_grad=True), tensor(189.6884, device='cuda:0', requires_grad=True), tensor(173.6589, device='cuda:0', requires_grad=True), tensor(160.0210, device='cuda:0', requires_grad=True)], 'word_idx': {'However,': 0, 'as': 1, 'an': 2, 'could': 3, 'have': 4, 'a': 5, 'to': 6, 'the': 7, 'there': 8, 'from': 9, 'was': 10, 'in': 11, 'with': 12, 'its': 13, 'of': 14, 'which': 15, 'would': 16, 'not': 17, 'him': 18, 'among': 19, 'those': 20, 'at': 21, 'who': 22, 'took': 23, 'his': 24, 'The': 25, 'were': 26, 'after': 27, 'released': 28, 'As': 29, 'no': 30, 'about': 31, 'and': 32, 'also': 33, 'part': 34, 'for': 35, 'he': 36, 'such': 37, 'can': 38, 'only': 39, 'be': 40, 'called': 41, 'some': 42, 'other': 43, 'He': 44, 'over': 45, 'year': 46, 'most': 47, 'than': 48, 'been': 49, 'but': 50, 'number': 51, 'these': 52, 'two': 53, 'last': 54, 'this': 55, 'all': 56, 'had': 57, 'one': 58, 'three': 59, 'made': 60, 'up': 61, 'by': 62, 'around': 63, 'between': 64, 'In': 65, 'early': 66, 'found': 67, 'set': 68, 'them': 69, 'on': 70, 'following': 71, 'second': 72, 'This': 73, 'various': 74, 'or': 75, 'are': 76, 'it': 77, 'is': 78, 'that': 79, 'has': 80, 'period': 81, 'before': 82, 'work': 83, 'through': 84, 'so': 85, 'Some': 86, 'their': 87, 'time': 88, 'show': 89, 'own': 90, 'end': 91, '\"The': 92, 'For': 93, 'John': 94, 'used': 95, 'when': 96, 'did': 97, 'still': 98, 'few': 99, 'out': 100, 'American': 101, 'published': 102, 'until': 103, 'One': 104, 'I': 105, 'more': 106, 'during': 107, 'less': 108, 'After': 109, 'A': 110, 'new': 111, 'first': 112, 'since': 113, 'major': 114, 'into': 115, 'modern': 116, 'described': 117, 'late': 118, 'often': 119, 'both': 120, 'human': 121, 'characters': 122, 'held': 123, 'named': 124, 'original': 125, 'even': 126, 'University': 127, 'without': 128, 'place': 129, 'name': 130, 'given': 131, 'May': 132, 'where': 133, 'then': 134, 'known': 135, 'like': 136, 'led': 137, 'several': 138, 'world': 139, 'British': 140, 'became': 141, 'make': 142, 'much': 143, 'include': 144, 'within': 145, 'including': 146, 'she': 147, 'same': 148, 'order': 149, 'while': 150, 'role': 151, 'During': 152, 'There': 153, 'It': 154, 'game': 155, 'North': 156, 'water': 157, 'now': 158, 'any': 159, 'very': 160, 'become': 161, 'series': 162, 'each': 163, 'under': 164, 'began': 165, '.': 166, 'land': 167, 'At': 168, 'next': 169, 'September': 170, 'single': 171, 'along': 172, 'large': 173, 'four': 174, 'will': 175, 'included': 176, '–': 177, 'common': 178, 'different': 179, 'recorded': 180, 'years': 181, 'many': 182, 'local': 183, 'well': 184, ',': 185, 'back': 186, 'being': 187, 'they': 188, 'tour': 189, 'form': 190, 'use': 191, 'do': 192, 'what': 193, 'may': 194, 'World': 195, 'against': 196, 'group': 197, 'public': 198, 'character': 199, 'if': 200, 'field': 201, 'because': 202, 'These': 203, 'They': 204, 'His': 205, 'later': 206, 'based': 207, 'another': 208, 'gas': 209, 'term': 210, 'United': 211, 'States': 212, 'US': 213, 'system': 214, 'people': 215, 'social': 216, 'five': 217, 'life': 218, 'government': 219, 'members': 220, 'considered': 221, 'using': 222, 'tax': 223, 'state': 224, 'due': 225, 'left': 226, 'On': 227, 'New': 228, 'June': 229, 'German': 230, 'No.': 231, '1': 232, 'her': 233, 'de': 234, 'October': 235, 'film': 236, 'band': 237, 'music': 238, 'played': 239, 'Moore': 240, 'Hall': 241, 'album': 242, 'album,': 243, 'Masaryk': 244, '<unk>': 245}, 'idx_word': {0: 'However,', 1: 'as', 2: 'an', 3: 'could', 4: 'have', 5: 'a', 6: 'to', 7: 'the', 8: 'there', 9: 'from', 10: 'was', 11: 'in', 12: 'with', 13: 'its', 14: 'of', 15: 'which', 16: 'would', 17: 'not', 18: 'him', 19: 'among', 20: 'those', 21: 'at', 22: 'who', 23: 'took', 24: 'his', 25: 'The', 26: 'were', 27: 'after', 28: 'released', 29: 'As', 30: 'no', 31: 'about', 32: 'and', 33: 'also', 34: 'part', 35: 'for', 36: 'he', 37: 'such', 38: 'can', 39: 'only', 40: 'be', 41: 'called', 42: 'some', 43: 'other', 44: 'He', 45: 'over', 46: 'year', 47: 'most', 48: 'than', 49: 'been', 50: 'but', 51: 'number', 52: 'these', 53: 'two', 54: 'last', 55: 'this', 56: 'all', 57: 'had', 58: 'one', 59: 'three', 60: 'made', 61: 'up', 62: 'by', 63: 'around', 64: 'between', 65: 'In', 66: 'early', 67: 'found', 68: 'set', 69: 'them', 70: 'on', 71: 'following', 72: 'second', 73: 'This', 74: 'various', 75: 'or', 76: 'are', 77: 'it', 78: 'is', 79: 'that', 80: 'has', 81: 'period', 82: 'before', 83: 'work', 84: 'through', 85: 'so', 86: 'Some', 87: 'their', 88: 'time', 89: 'show', 90: 'own', 91: 'end', 92: '\"The', 93: 'For', 94: 'John', 95: 'used', 96: 'when', 97: 'did', 98: 'still', 99: 'few', 100: 'out', 101: 'American', 102: 'published', 103: 'until', 104: 'One', 105: 'I', 106: 'more', 107: 'during', 108: 'less', 109: 'After', 110: 'A', 111: 'new', 112: 'first', 113: 'since', 114: 'major', 115: 'into', 116: 'modern', 117: 'described', 118: 'late', 119: 'often', 120: 'both', 121: 'human', 122: 'characters', 123: 'held', 124: 'named', 125: 'original', 126: 'even', 127: 'University', 128: 'without', 129: 'place', 130: 'name', 131: 'given', 132: 'May', 133: 'where', 134: 'then', 135: 'known', 136: 'like', 137: 'led', 138: 'several', 139: 'world', 140: 'British', 141: 'became', 142: 'make', 143: 'much', 144: 'include', 145: 'within', 146: 'including', 147: 'she', 148: 'same', 149: 'order', 150: 'while', 151: 'role', 152: 'During', 153: 'There', 154: 'It', 155: 'game', 156: 'North', 157: 'water', 158: 'now', 159: 'any', 160: 'very', 161: 'become', 162: 'series', 163: 'each', 164: 'under', 165: 'began', 166: '.', 167: 'land', 168: 'At', 169: 'next', 170: 'September', 171: 'single', 172: 'along', 173: 'large', 174: 'four', 175: 'will', 176: 'included', 177: '–', 178: 'common', 179: 'different', 180: 'recorded', 181: 'years', 182: 'many', 183: 'local', 184: 'well', 185: ',', 186: 'back', 187: 'being', 188: 'they', 189: 'tour', 190: 'form', 191: 'use', 192: 'do', 193: 'what', 194: 'may', 195: 'World', 196: 'against', 197: 'group', 198: 'public', 199: 'character', 200: 'if', 201: 'field', 202: 'because', 203: 'These', 204: 'They', 205: 'His', 206: 'later', 207: 'based', 208: 'another', 209: 'gas', 210: 'term', 211: 'United', 212: 'States', 213: 'US', 214: 'system', 215: 'people', 216: 'social', 217: 'five', 218: 'life', 219: 'government', 220: 'members', 221: 'considered', 222: 'using', 223: 'tax', 224: 'state', 225: 'due', 226: 'left', 227: 'On', 228: 'New', 229: 'June', 230: 'German', 231: 'No.', 232: '1', 233: 'her', 234: 'de', 235: 'October', 236: 'film', 237: 'band', 238: 'music', 239: 'played', 240: 'Moore', 241: 'Hall', 242: 'album', 243: 'album,', 244: 'Masaryk', 245: '<unk>'}}\n"
          ]
        }
      ],
      "source": [
        "r = torch.load('models/model_cbow_negTrue.pth')\n",
        "print(r)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a01861876d126b30cd1b77ced19e532f706bc03e1154ffd933be16f6f668bcc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
