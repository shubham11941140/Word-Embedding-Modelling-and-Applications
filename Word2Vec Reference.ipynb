{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Gjyw2be9rQ"
      },
      "source": [
        "### Training Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PXSZLBLAKYlL"
      },
      "outputs": [],
      "source": [
        "CBOW_N_WORDS = 4\n",
        "SKIPGRAM_N_WORDS = 4\n",
        "\n",
        "MIN_WORD_FREQUENCY = 50\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "EMBED_DIMENSION = 300\n",
        "EMBED_MAX_NORM = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5JBPaieqaodL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#from utils.constants import EMBED_DIMENSION, EMBED_MAX_NORM\n",
        "\n",
        "class SkipGram_Model(nn.Module):\n",
        "  \n",
        "    def __init__(self, vocab_size: int):\n",
        "        super(SkipGram_Model, self).__init__()\n",
        "        self.embeddings = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=EMBED_DIMENSION,\n",
        "            max_norm=EMBED_MAX_NORM,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=EMBED_DIMENSION,\n",
        "            out_features=vocab_size,\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs_):\n",
        "        x = self.embeddings(inputs_)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "model=SkipGram_Model(4099)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h20URx64aogQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "\n",
        "def get_model_class(model_name: str):\n",
        "    if model_name == \"skipgram\":\n",
        "        return SkipGram_Model\n",
        "    else:\n",
        "        raise ValueError(\"Choose model_name from:  skipgram\")\n",
        "        return\n",
        "\n",
        "\n",
        "def get_optimizer_class(name: str):\n",
        "    if name == \"Adam\":\n",
        "        return optim.Adam\n",
        "    else:\n",
        "        raise ValueError(\"Choose optimizer from: Adam\")\n",
        "        return\n",
        "    \n",
        "\n",
        "def get_lr_scheduler(optimizer, total_epochs: int, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Scheduler to linearly decrease learning rate, \n",
        "    so thatlearning rate after the last epoch is 0.\n",
        "    \"\"\"\n",
        "    lr_lambda = lambda epoch: (total_epochs - epoch) / total_epochs\n",
        "    lr_scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda, verbose=verbose)\n",
        "    return lr_scheduler\n",
        "\n",
        "\n",
        "def save_config(config: dict, model_dir: str):\n",
        "    \"\"\"Save config file to `model_dir` directory\"\"\"\n",
        "    config_path = os.path.join(model_dir, \"config.yaml\")\n",
        "    with open(config_path, \"w\") as stream:\n",
        "        yaml.dump(config, stream)\n",
        "        \n",
        "        \n",
        "def save_vocab(vocab, model_dir: str):\n",
        "    \"\"\"Save vocab file to `model_dir` directory\"\"\"\n",
        "    vocab_path = os.path.join(model_dir, \"vocab.pt\")\n",
        "    torch.save(vocab, vocab_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TIo0opwbcTqk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from functools import partial\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103\n",
        "\n",
        "\n",
        "\n",
        "def get_english_tokenizer():\n",
        "   \n",
        "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def get_data_iterator(ds_name, ds_type, data_dir):\n",
        "    if ds_name == \"WikiText2\":\n",
        "        data_iter = WikiText2(root=data_dir, split=(ds_type))\n",
        "    elif ds_name == \"WikiText103\":\n",
        "        data_iter = WikiText103(root=data_dir, split=(ds_type))\n",
        "    else:\n",
        "        raise ValueError(\"Choose dataset from: WikiText2, WikiText103\")\n",
        "    data_iter = to_map_style_dataset(data_iter)\n",
        "    return data_iter\n",
        "\n",
        "\n",
        "def build_vocab(data_iter, tokenizer):\n",
        "    \n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter),\n",
        "        specials=[\"<unk>\"],\n",
        "        min_freq=MIN_WORD_FREQUENCY,\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "    return vocab\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def collate_skipgram(batch, text_pipeline):\n",
        "    \"\"\"\n",
        "    Collate_fn for Skip-Gram model to be used with Dataloader.\n",
        "    `batch` is expected to be list of text paragrahs.\n",
        "    \n",
        "    Context is represented as N=SKIPGRAM_N_WORDS past words \n",
        "    and N=SKIPGRAM_N_WORDS future words.\n",
        "    \n",
        "    Long paragraphs will be truncated to contain\n",
        "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
        "    \n",
        "    Each element in `batch_input` is a middle word.\n",
        "    Each element in `batch_output` is a context word.\n",
        "    \"\"\"\n",
        "    batch_input, batch_output = [], []\n",
        "    for text in batch:\n",
        "        text_tokens_ids = text_pipeline(text)\n",
        "\n",
        "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
        "            continue\n",
        "\n",
        "        if MAX_SEQUENCE_LENGTH:\n",
        "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
        "\n",
        "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
        "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
        "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
        "            outputs = token_id_sequence\n",
        "\n",
        "            for output in outputs:\n",
        "                batch_input.append(input_)\n",
        "                batch_output.append(output)\n",
        "\n",
        "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "    return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    model_name, ds_name, ds_type, data_dir, batch_size, shuffle, vocab=None\n",
        "):\n",
        "\n",
        "    data_iter = get_data_iterator(ds_name, ds_type, data_dir)\n",
        "    tokenizer = get_english_tokenizer()\n",
        "\n",
        "    if not vocab:\n",
        "        vocab = build_vocab(data_iter, tokenizer)\n",
        "        \n",
        "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "    if model_name == \"skipgram\":\n",
        "        collate_fn = collate_skipgram\n",
        "    else:\n",
        "        raise ValueError(\"Choose model from: skipgram\")\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        data_iter,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),\n",
        "    )\n",
        "    return dataloader, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Kmamm5vcaoab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"Main class for model training\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        epochs,\n",
        "        train_dataloader,\n",
        "        train_steps,\n",
        "        val_dataloader,\n",
        "        val_steps,\n",
        "        checkpoint_frequency,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        lr_scheduler,\n",
        "        device,\n",
        "        model_dir,\n",
        "        model_name,\n",
        "    ):  \n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.train_steps = train_steps\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.val_steps = val_steps\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.checkpoint_frequency = checkpoint_frequency\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.device = device\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.loss = {\"train\": [], \"val\": []}\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            self._train_epoch()\n",
        "            self._validate_epoch()\n",
        "            print(\n",
        "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
        "                    epoch + 1,\n",
        "                    self.epochs,\n",
        "                    self.loss[\"train\"][-1],\n",
        "                    self.loss[\"val\"][-1],\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "            if self.checkpoint_frequency:\n",
        "                self._save_checkpoint(epoch)\n",
        "\n",
        "    def _train_epoch(self):\n",
        "        self.model.train()\n",
        "        running_loss = []\n",
        "\n",
        "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
        "            inputs = batch_data[0].to(self.device)\n",
        "            labels = batch_data[1].to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss.append(loss.item())\n",
        "\n",
        "            if i == self.train_steps:\n",
        "                break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"train\"].append(epoch_loss)\n",
        "\n",
        "    def _validate_epoch(self):\n",
        "        self.model.eval()\n",
        "        running_loss = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
        "                inputs = batch_data[0].to(self.device)\n",
        "                labels = batch_data[1].to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss.append(loss.item())\n",
        "\n",
        "                if i == self.val_steps:\n",
        "                    break\n",
        "\n",
        "        epoch_loss = np.mean(running_loss)\n",
        "        self.loss[\"val\"].append(epoch_loss)\n",
        "\n",
        "    def _save_checkpoint(self, epoch):\n",
        "        \"\"\"Save model checkpoint to `self.model_dir` directory\"\"\"\n",
        "        epoch_num = epoch + 1\n",
        "        if epoch_num % self.checkpoint_frequency == 0:\n",
        "            model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
        "            model_path = os.path.join(self.model_dir, model_path)\n",
        "            torch.save(self.model, model_path)\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save final model to `self.model_dir` directory\"\"\"\n",
        "        model_path = os.path.join(self.model_dir, \"model.pt\")\n",
        "        torch.save(self.model, model_path)\n",
        "\n",
        "    def save_loss(self):\n",
        "        \"\"\"Save train/val loss as json file to `self.model_dir` directory\"\"\"\n",
        "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
        "        with open(loss_path, \"w\") as fp:\n",
        "            json.dump(self.loss, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlrRRcZRafSM",
        "outputId": "eaf476d3-34d8-44aa-ff4c-f2cae33eab37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 4099\n",
            "Adjusting learning rate of group 0 to 2.5000e-02.\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import yaml\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchdata.datapipes.iter import IterableWrapper\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(config):\n",
        "    os.makedirs(config[\"model_dir\"])\n",
        "    \n",
        "    train_dataloader, vocab = get_dataloader_and_vocab(\n",
        "        model_name=config[\"model_name\"],\n",
        "        ds_name=config[\"dataset\"],\n",
        "        ds_type=\"train\",\n",
        "        data_dir=config[\"data_dir\"],\n",
        "        batch_size=config[\"train_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "        vocab=None,\n",
        "    )\n",
        "\n",
        "    val_dataloader, _ = get_dataloader_and_vocab(\n",
        "        model_name=config[\"model_name\"],\n",
        "        ds_name=config[\"dataset\"],\n",
        "        ds_type=\"valid\",\n",
        "        data_dir=config[\"data_dir\"],\n",
        "        batch_size=config[\"val_batch_size\"],\n",
        "        shuffle=config[\"shuffle\"],\n",
        "        vocab=vocab,\n",
        "    )\n",
        "\n",
        "    vocab_size = len(vocab.get_stoi())\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    model_class = get_model_class(config[\"model_name\"])\n",
        "    model = model_class(vocab_size=vocab_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer_class = get_optimizer_class(config[\"optimizer\"])\n",
        "    optimizer = optimizer_class(model.parameters(), lr=config[\"learning_rate\"])\n",
        "    lr_scheduler = get_lr_scheduler(optimizer, config[\"epochs\"], verbose=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        epochs=config[\"epochs\"],\n",
        "        train_dataloader=train_dataloader,\n",
        "        train_steps=config[\"train_steps\"],\n",
        "        val_dataloader=val_dataloader,\n",
        "        val_steps=config[\"val_steps\"],\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        checkpoint_frequency=config[\"checkpoint_frequency\"],\n",
        "        lr_scheduler=lr_scheduler,\n",
        "        device=device,\n",
        "        model_dir=config[\"model_dir\"],\n",
        "        model_name=config[\"model_name\"],\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "    trainer.save_model()\n",
        "    trainer.save_loss()\n",
        "    save_vocab(vocab, config[\"model_dir\"])\n",
        "    save_config(config, config[\"model_dir\"])\n",
        "    print(\"Model artifacts saved to folder:\", config[\"model_dir\"])\n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    '''parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--config', type=str, required=True, help='path to yaml config')\n",
        "    args = parser.parse_args()'''\n",
        "    \n",
        "    with open('config.yaml', 'r') as stream:\n",
        "        config = yaml.safe_load(stream)\n",
        "    train(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owEcaoLdbOIg",
        "outputId": "374d43e5-c52e-44b2-9d1c-e74addad9733"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hms: 0.507\n",
            "victoria: 0.506\n",
            "king: 0.502\n",
            "frederick: 0.497\n",
            "charles: 0.494\n",
            "elizabeth: 0.485\n",
            "edward: 0.482\n",
            "george: 0.471\n",
            "daughter: 0.461\n",
            "son: 0.431\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "#model=torch.load(\"/content/weights/cbow_WikiText2'/model.pt\")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/DS504/model1.pth\"))\n",
        "vocab = torch.load(\"/content/drive/MyDrive/DS504/vocab.pth\")\n",
        "#Getting Embeddings\n",
        "\n",
        "embeddings = list(model.parameters())[0]\n",
        "embeddings = embeddings.cpu().detach().numpy()\n",
        "\n",
        "# normalization\n",
        "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "norms = np.reshape(norms, (len(norms), 1))\n",
        "embeddings_norm = embeddings / norms\n",
        "embeddings_norm.shape\n",
        "#Visualization with t-SNE\n",
        "# get embeddings\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "\n",
        "# t-SNE transform\n",
        "tsne = TSNE(n_components=2)\n",
        "embeddings_df_trans = tsne.fit_transform(embeddings_df)\n",
        "embeddings_df_trans = pd.DataFrame(embeddings_df_trans)\n",
        "\n",
        "# get token order\n",
        "embeddings_df_trans.index = vocab.get_itos()\n",
        "\n",
        "# if token is a number\n",
        "is_numeric = embeddings_df_trans.index.str.isnumeric()\n",
        "color = np.where(is_numeric, \"green\", \"black\")\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=embeddings_df_trans[0],\n",
        "        y=embeddings_df_trans[1],\n",
        "        mode=\"text\",\n",
        "        text=embeddings_df_trans.index,\n",
        "        textposition=\"middle center\",\n",
        "        textfont=dict(color=color),\n",
        "    )\n",
        ")\n",
        "fig.write_html(\"/content/drive/MyDrive/DS504/word2vec_visualization.html\")\n",
        "#Find Similar Words\n",
        "def get_top_similar(word: str, topN: int = 10):\n",
        "    word_id = vocab[word]\n",
        "    if word_id == 0:\n",
        "        print(\"Out of vocabulary word\")\n",
        "        return\n",
        "\n",
        "    word_vec = embeddings_norm[word_id]\n",
        "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "    dists = np.matmul(embeddings_norm, word_vec).flatten()\n",
        "    topN_ids = np.argsort(-dists)[1 : topN + 1]\n",
        "\n",
        "    topN_dict = {}\n",
        "    for sim_word_id in topN_ids:\n",
        "        sim_word = vocab.lookup_token(sim_word_id)\n",
        "        topN_dict[sim_word] = dists[sim_word_id]\n",
        "    return topN_dict\n",
        "for word, sim in get_top_similar(\"queen\").items():\n",
        "    print(\"{}: {:.3f}\".format(word, sim))\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boDSwA4NcYPw",
        "outputId": "ba90fcae-48b3-49c7-8251-2066c3107069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "queen: 0.731\n",
            "king: 0.438\n",
            "man: 0.426\n",
            "edward: 0.390\n",
            "hms: 0.371\n"
          ]
        }
      ],
      "source": [
        "\n",
        " \n",
        "#Vector Equations\n",
        "emb1 = embeddings[vocab[\"man\"]]\n",
        "emb2 = embeddings[vocab[\"woman\"]]\n",
        "emb3 = embeddings[vocab[\"queen\"]]\n",
        "\n",
        "emb4 = emb1 - emb2 + emb3\n",
        "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
        "emb4 = emb4 / emb4_norm\n",
        "\n",
        "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
        "\n",
        "top5 = np.argsort(-dists)[:5]\n",
        "\n",
        "for word_id in top5:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiL50baMdgHj",
        "outputId": "157cdae2-6255-4ecc-c416-5d58aa896bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<unk>: 1.000\n",
            "bone: 0.405\n",
            "simple: 0.386\n",
            "bishop: 0.361\n",
            "jean: 0.357\n"
          ]
        }
      ],
      "source": [
        "#Vector Equations\n",
        "emb1 = embeddings[vocab[\"milk\"]]\n",
        "emb2 = embeddings[vocab[\"pant\"]]\n",
        "emb3 = embeddings[vocab[\"tea\"]]\n",
        "\n",
        "emb4 = emb1 - emb2 + emb3\n",
        "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
        "emb4 = emb4 / emb4_norm\n",
        "\n",
        "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
        "\n",
        "top5 = np.argsort(-dists)[:5]\n",
        "\n",
        "for word_id in top5:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1_RfS-6DnSI"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model1.pth\")\n",
        "torch.save(\"vocab.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DdqCUwOfKVN"
      },
      "source": [
        "### Pretrained Word2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "UweUKlfOz3M2",
        "outputId": "915e29bf-8edd-41af-d189-212cd1c4fde2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "            ['this', 'is', 'the', 'second', 'sentence'],\n",
        "            ['yet', 'another', 'sentence'],\n",
        "            ['one', 'more', 'sentence'],\n",
        "            ['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model_1 = Word2Vec(sentences, size=300, min_count=1)\n",
        "\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = model_1[model_1.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model_1.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glvPZeg7f4Q9",
        "outputId": "586c62c2-1ab7-424b-f6ed-22df1ce8a176"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('another', 0.12716959416866302),\n",
              " ('one', 0.06272583454847336),\n",
              " ('sentence', 0.060022421181201935),\n",
              " ('is', 0.038779787719249725),\n",
              " ('the', 0.0308675616979599),\n",
              " ('second', 0.01855158619582653),\n",
              " ('for', 0.013133496046066284),\n",
              " ('and', 0.003790736198425293),\n",
              " ('more', 0.0032859928905963898),\n",
              " ('yet', -0.008202701807022095)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_1.most_similar(positive=['final'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXzvHLEFot1W"
      },
      "source": [
        "### Sentiment analysis using word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgO6qYySIsDh",
        "outputId": "33261e7a-dfe0-47ba-8570-467f29b03f85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import nltk.data,logging, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def reviewToSentences(review, tokenizer, remove_stopwords=False):\n",
        "    rawSentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in rawSentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(reviewToWordList(raw_sentence, remove_stopwords))\n",
        "    return sentences\n",
        "\n",
        "def reviewToWordList(review, remove_stopwords=False):\n",
        "    reviewText = BeautifulSoup(review).get_text() # Remove HTML\n",
        "    reviewText = re.sub(\"[^a-zA-Z]\",\" \", reviewText)    # Remove non-letters\n",
        "    words = reviewText.lower().split()    # Convert words to lower case and split them\n",
        "\n",
        "    # 4. Optionally remove stop words (\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return(words)\n",
        "\n",
        "# Function to average all of the word vectors in a given paragraph\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "\n",
        "    # initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "\n",
        "    nwords = 0.\n",
        "\n",
        "    # Index2word is a list that contains the names of the words in\n",
        "    # the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "\n",
        "    # Loop over each word in the review and, if it is in the model's\n",
        "    # vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "\n",
        "    # Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n",
        "# Given a set of reviews (each one a list of words), calculate the average feature vector for each one and return a 2D numpy array\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    # Initialize a counter\n",
        "    counter = 0.\n",
        "\n",
        "    # Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "\n",
        "    # Loop through the reviews\n",
        "    for review in reviews:\n",
        "\n",
        "        if counter%1000. == 0.:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "\n",
        "        # Call the function (defined above) that makes average feature vectors\n",
        "        reviewFeatureVecs[int(counter)] = makeFeatureVec(review, model, num_features)\n",
        "        counter = counter + 1.\n",
        "\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "def getCleanReviews(reviews):\n",
        "    clean_reviews = []\n",
        "    for review in reviews[\"review\"]:\n",
        "        clean_reviews.append(reviewToWordList(review, remove_stopwords=True))\n",
        "    return clean_reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1AUeg3wbMWd",
        "outputId": "1edc80c3-0dc7-4ab0-bf9d-6ca15690f7cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
            "\n",
            "Parsing sentences from training set\n",
            "Parsing sentences from unlabeled set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning:\n",
            "\n",
            "\"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning:\n",
            "\n",
            "\"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:336: UserWarning:\n",
            "\n",
            "\"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Word2Vec model...\n"
          ]
        }
      ],
      "source": [
        "# Read data from files\n",
        "train = pd.read_csv( '/content/drive/MyDrive/DS504/labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
        "test = pd.read_csv( '/content/drive/MyDrive/DS504/testData.tsv', header=0, delimiter=\"\\t\", quoting=3 )\n",
        "unlabeled_train = pd.read_csv( \"/content/drive/MyDrive/DS504/unlabeledTrainData.tsv\", header=0,  delimiter=\"\\t\", quoting=3 )\n",
        "\n",
        "    # Verify the number of reviews that were read (100,000 in total)\n",
        "print( \"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
        "          \"and %d unlabeled reviews\\n\" % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size ))\n",
        "\n",
        "    # Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print (\"Parsing sentences from training set\")\n",
        "for review in train[\"review\"]:\n",
        "        sentences += reviewToSentences(review, tokenizer)\n",
        "\n",
        "print (\"Parsing sentences from unlabeled set\")\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "        sentences += reviewToSentences(review, tokenizer)\n",
        "\n",
        "    # Set parameters and train the word2vec model\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
        "                        level=logging.INFO)\n",
        "\n",
        "    # Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality\n",
        "min_word_count = 40   # Minimum word count\n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size\n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "    # Initialize and train the model (this will take some time)\n",
        "print (\"Training Word2Vec model...\")\n",
        "model=Word2Vec(sentences,workers=num_workers,size=num_features,min_count=min_word_count,window=context,sample=downsampling,seed=1)\n",
        "\n",
        "    # If you don't plan to train the model any further, calling\n",
        "    # init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "    #SAVE THE MODEL\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYlddq0ge1TD",
        "outputId": "fb4bba1f-9654-4604-f6f0-425eaaeec8a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating average feature vecs for training reviews\n",
            "Review 0 of 25000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review 1000 of 25000\n",
            "Review 2000 of 25000\n",
            "Review 3000 of 25000\n",
            "Review 4000 of 25000\n",
            "Review 5000 of 25000\n",
            "Review 6000 of 25000\n",
            "Review 7000 of 25000\n",
            "Review 8000 of 25000\n",
            "Review 9000 of 25000\n",
            "Review 10000 of 25000\n",
            "Review 11000 of 25000\n",
            "Review 12000 of 25000\n",
            "Review 13000 of 25000\n",
            "Review 14000 of 25000\n",
            "Review 15000 of 25000\n",
            "Review 16000 of 25000\n",
            "Review 17000 of 25000\n",
            "Review 18000 of 25000\n",
            "Review 19000 of 25000\n",
            "Review 20000 of 25000\n",
            "Review 21000 of 25000\n",
            "Review 22000 of 25000\n",
            "Review 23000 of 25000\n",
            "Review 24000 of 25000\n",
            "Creating average feature vecs for test reviews\n",
            "Review 0 of 25000\n",
            "Review 1000 of 25000\n",
            "Review 2000 of 25000\n",
            "Review 3000 of 25000\n",
            "Review 4000 of 25000\n",
            "Review 5000 of 25000\n",
            "Review 6000 of 25000\n",
            "Review 7000 of 25000\n",
            "Review 8000 of 25000\n",
            "Review 9000 of 25000\n",
            "Review 10000 of 25000\n",
            "Review 11000 of 25000\n",
            "Review 12000 of 25000\n",
            "Review 13000 of 25000\n",
            "Review 14000 of 25000\n",
            "Review 15000 of 25000\n",
            "Review 16000 of 25000\n",
            "Review 17000 of 25000\n",
            "Review 18000 of 25000\n",
            "Review 19000 of 25000\n",
            "Review 20000 of 25000\n",
            "Review 21000 of 25000\n",
            "Review 22000 of 25000\n",
            "Review 23000 of 25000\n",
            "Review 24000 of 25000\n",
            "Fitting a random forest to labeled training data...\n",
            "Wrote Word2Vec_AverageVectors.csv\n"
          ]
        }
      ],
      "source": [
        " # TEST THE MODEL\n",
        "model.doesnt_match(\"man woman child kitchen\".split())\n",
        "model.doesnt_match(\"france england germany berlin\".split())\n",
        "model.doesnt_match(\"paris berlin london austria\".split())\n",
        "model.most_similar(\"man\")\n",
        "model.most_similar(\"queen\")\n",
        "model.most_similar(\"awful\")\n",
        "nltk.download('stopwords')\n",
        "    # Create average vectors for the training and test sets\n",
        "print (\"Creating average feature vecs for training reviews\")\n",
        "trainDataVecs = getAvgFeatureVecs( getCleanReviews(train), model, num_features )\n",
        "\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "testDataVecs = getAvgFeatureVecs( getCleanReviews(test), model, num_features )\n",
        "\n",
        "    # Fit a random forest to the training set, then make predictions\n",
        "forest = RandomForestClassifier( n_estimators = 100 )\n",
        "\n",
        "print (\"Fitting a random forest to labeled training data...\")\n",
        "forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
        "\n",
        "    # Test & extract results\n",
        "result = forest.predict( testDataVecs )\n",
        "\n",
        "    # Write the test results\n",
        "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
        "output.to_csv( \"/content/OUTPUT_Word2Vec_AverageVectors.csv\", index=False, quoting=3 )\n",
        "print (\"Wrote Word2Vec_AverageVectors.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWFoGEUKo5Vv"
      },
      "source": [
        "### Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q_dZuJaS1nJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5bX7Z9aSFyy"
      },
      "outputs": [],
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "\n",
        "\n",
        "\n",
        "sentence = 'Deemed universities charge huge fees'\n",
        "\n",
        "# Initialize StanfordDependency Parser from the path\n",
        "parser = StanfordDependencyParser(path_to_jar = \"D:\\DS504 Natural Language Processing\\DS504\\stanford-corenlp-4.5.1\\stanford-corenlp-4.5.1.jar\", path_to_models_jar =  \"D:\\DS504 Natural Language Processing\\DS504\\stanford-corenlp-4.5.1\\stanford-corenlp-4.5.1-models.jar\",    )\n",
        "\n",
        "# Parse the sentence\n",
        "result = parser.raw_parse(sentence)\n",
        "dependency = result.__next__()\n",
        "\n",
        "\n",
        "print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\".format('Head', 'Head POS','Relation','Dependent', 'Dependent POS'))\n",
        "print (\"-\" * 75)\n",
        "  \n",
        "# Use dependency.triples() to extract the dependency triples in the form\n",
        "# ((head word, head POS), relation, (dependent word, dependent POS))  \n",
        "for dep in list(dependency.triples()):\n",
        "  print (\"{:<15} | {:<10} | {:<10} | {:<15} | {:<10}\"\n",
        "         .format(str(dep[0][0]),str(dep[0][1]), str(dep[1]), str(dep[2][0]),str(dep[2][1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzPmYP2ZWPb3"
      },
      "outputs": [],
      "source": [
        "# importing networkx\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "# Using reverse() to reverse the direction of edges as nx_graph() returns inverted edges\n",
        "G = dependency.nx_graph().reverse()\n",
        "# nx_graph() returns numeric node labels starting from 1\n",
        "# Create a dictionary to map numeric nodes and words in the sentence\n",
        "words = sentence.split(\" \")\n",
        "labels = {index + 1: words[index] for index in range(len(words))}\n",
        "nx.draw(G, with_labels=True, labels=labels, node_size=2500, node_color='#B5EAD7', font_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMD1l9JHWTnY"
      },
      "outputs": [],
      "source": [
        "!pip install graphviz\n",
        "from graphviz import Source\n",
        "dot_def = dependency.to_dot()\n",
        "'''\n",
        "The below lines require graphviz executables to be installed to your system. If that does not work, print the dot_def object and paste the output on http://graphs.grevian.org/graph/\n",
        "'''\n",
        "source = Source(dot_def, filename=\"dependency_graph\", format=\"png\")\n",
        "source.view()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.4 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a01861876d126b30cd1b77ced19e532f706bc03e1154ffd933be16f6f668bcc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
